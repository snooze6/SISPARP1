[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1bd1ab0
[0] MPI startup(): Multi-threaded optimized library
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=11b9ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=23abab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=117cab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=d4bab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=bcfab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1951ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=d10ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=17ebab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=2126ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=d8dab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1100ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=e5cab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=fc7ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=b99ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=24dbab0
[8] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[9] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[7] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[5] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[3] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[7] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[7] MPI startup(): dapl data transfer mode
[6] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[8] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[8] MPI startup(): dapl data transfer mode
[9] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[9] MPI startup(): dapl data transfer mode
[4] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[2] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[5] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[5] MPI startup(): dapl data transfer mode
[3] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[3] MPI startup(): dapl data transfer mode
[0] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[6] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[6] MPI startup(): dapl data transfer mode
[1] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[2] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[2] MPI startup(): dapl data transfer mode
[4] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[4] MPI startup(): dapl data transfer mode
[11] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[14] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[0] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[0] MPI startup(): dapl data transfer mode
[1] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[1] MPI startup(): dapl data transfer mode
[11] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[11] MPI startup(): dapl data transfer mode
[14] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[14] MPI startup(): dapl data transfer mode
[13] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[12] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[10] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[13] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[13] MPI startup(): dapl data transfer mode
[15] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[12] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[12] MPI startup(): dapl data transfer mode
[10] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[10] MPI startup(): dapl data transfer mode
[15] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[15] MPI startup(): dapl data transfer mode
[5] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[5] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[4] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[4] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[6] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[6] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[3] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[3] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[7] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[7] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[8] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[8] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[9] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[9] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[2] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[2] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[11] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[11] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[10] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[10] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[1] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[1] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[13] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[13] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[12] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[12] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[14] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[14] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[0] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[0] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[15] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[15] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[0] MPI startup(): Rank    Pid      Node name  Pin cpu
[0] MPI startup(): 0       22303    c7309      +1
[0] MPI startup(): 1       26111    c7310      +1
[0] MPI startup(): 2       30219    c7311      +1
[0] MPI startup(): 3       8298     c7312      +1
[0] MPI startup(): 4       26197    c7313      +1
[0] MPI startup(): 5       21179    c7314      +1
[0] MPI startup(): 6       13332    c7315      +1
[0] MPI startup(): 7       20183    c7316      +1
[0] MPI startup(): 8       13880    c7317      +1
[0] MPI startup(): 9       4733     c7318      +1
[0] MPI startup(): 10      31343    c7319      +1
[0] MPI startup(): 11      21581    c7320      +1
[0] MPI startup(): 12      7109     c7321      +1
[0] MPI startup(): 13      25215    c7322      +1
[0] MPI startup(): 14      17274    c7323      +1
[0] MPI startup(): 15      14671    c7324      +1
Hello world from processor 13 out of 16 processors
Hello world from processor 11 out of 16 processors
Hello world from processor 7 out of 16 processors
Hello world from processor 1 out of 16 processors
Hello world from processor 9 out of 16 processors
Hello world from processor 10 out of 16 processors
Hello world from processor 6 out of 16 processors
Hello world from processor 14 out of 16 processors
Hello world from processor 3 out of 16 processors
Hello world from processor 5 out of 16 processors
Hello world from processor 4 out of 16 processors
Hello world from processor 15 out of 16 processors
Hello world from processor 2 out of 16 processors
Hello world from processor 8 out of 16 processors
Hello world from processor 12 out of 16 processors
[0] MPI startup(): I_MPI_DEBUG=5
Hello world from processor 0 out of 16 processors
