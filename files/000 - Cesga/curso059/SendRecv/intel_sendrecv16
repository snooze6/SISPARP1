[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=180fab0
[0] MPI startup(): Multi-threaded optimized library
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=18ffab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=7a8ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=68fab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1db3ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1a67ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=13acab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=229bab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=ce8ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=2569ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1acfab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=14efab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=824ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=a17ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=257fab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=8b0ab0
[4] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[4] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[4] MPI startup(): dapl data transfer mode
[3] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[3] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[3] MPI startup(): dapl data transfer mode
[0] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[1] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[13] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[1] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[1] MPI startup(): dapl data transfer mode
[0] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[0] MPI startup(): dapl data transfer mode
[8] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[13] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[13] MPI startup(): dapl data transfer mode
[8] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[8] MPI startup(): dapl data transfer mode
[12] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[12] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[12] MPI startup(): dapl data transfer mode
[10] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[6] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[10] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[10] MPI startup(): dapl data transfer mode
[6] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[6] MPI startup(): dapl data transfer mode
[5] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[14] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[15] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[5] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[5] MPI startup(): dapl data transfer mode
[9] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[11] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[7] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[14] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[14] MPI startup(): dapl data transfer mode
[15] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[15] MPI startup(): dapl data transfer mode
[9] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[9] MPI startup(): dapl data transfer mode
[11] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[11] MPI startup(): dapl data transfer mode
[7] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[7] MPI startup(): dapl data transfer mode
[2] DAPL startup(): trying to open DAPL provider from I_MPI_DAPL_PROVIDER: ofa-v2-mlx4_0-1u
[2] MPI startup(): DAPL provider ofa-v2-mlx4_0-1u
[2] MPI startup(): dapl data transfer mode
[4] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[4] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[6] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[6] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[5] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[5] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[7] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[7] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[8] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[8] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[9] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[9] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[3] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[3] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[10] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[10] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[1] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[1] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[12] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[12] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[14] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[14] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[11] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[11] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[13] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[13] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[2] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[2] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[0] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[0] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[15] MPID_nem_init_dapl_coll_fns(): User set DAPL collective mask = 0000
[15] MPID_nem_init_dapl_coll_fns(): Effective DAPL collective mask = 0000
[0] MPI startup(): Rank    Pid      Node name  Pin cpu
[0] MPI startup(): 0       16267    c7001      +1
[0] MPI startup(): 1       26301    c7002      +1
[0] MPI startup(): 2       18883    c7003      +1
[0] MPI startup(): 3       2629     c7004      +1
[0] MPI startup(): 4       17596    c7005      +1
[0] MPI startup(): 5       786      c7010      +1
[0] MPI startup(): 6       21640    c7011      +1
[0] MPI startup(): 7       30668    c7013      +1
[0] MPI startup(): 8       26213    c7014      +1
[0] MPI startup(): 9       19450    c7015      +1
[0] MPI startup(): 10      27073    c7016      +1
[0] MPI startup(): 11      12585    c7019      +1
[0] MPI startup(): 12      19595    c7020      +1
[0] MPI startup(): 13      29366    c7021      +1
[0] MPI startup(): 14      25149    c7022      +1
[0] MPI startup(): 15      11782    c7024      +1
[0] MPI startup(): I_MPI_DEBUG=5
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1 Update 1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Mon Oct 31 11:14:53 2016
# Machine               : x86_64
# System                : Linux
# Release               : 2.6.32-573.12.1.el6.x86_64
# Version               : #1 SMP Mon Nov 23 12:55:32 EST 2015
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# /opt/cesga/intel/impi/5.1.3.223/bin64/IMB-MPI1 sendrecv

# Minimum message length in bytes:   0
# Maximum message length in bytes:   4194304
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Sendrecv

#-----------------------------------------------------------------------------
# Benchmarking Sendrecv 
# #processes = 2 
# ( 14 additional processes waiting in MPI_Barrier)
#-----------------------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]   Mbytes/sec
            0         1000         1.62         1.62         1.62         0.00
            1         1000         1.63         1.63         1.63         1.17
            2         1000         1.64         1.64         1.64         2.33
            4         1000         1.64         1.64         1.64         4.64
            8         1000         1.62         1.62         1.62         9.43
           16         1000         1.62         1.62         1.62        18.80
           32         1000         2.64         2.64         2.64        23.08
           64         1000         2.62         2.63         2.63        46.47
          128         1000         2.67         2.67         2.67        91.54
          256         1000         2.86         2.87         2.86       170.38
          512         1000         2.94         2.94         2.94       332.28
         1024         1000         3.26         3.26         3.26       599.49
         2048         1000         3.89         3.89         3.89      1003.92
         4096         1000         4.67         4.67         4.67      1672.95
         8192         1000         6.20         6.20         6.20      2520.52
        16384         1000         7.98         7.99         7.99      3912.13
        32768         1000        11.43        11.44        11.43      5465.66
        65536          640        17.45        17.45        17.45      7163.32
       131072          320        34.90        34.91        34.90      7161.95
       262144          160        56.48        56.49        56.48      8851.54
       524288           80       119.45       119.47       119.46      8369.98
      1048576           40       226.10       226.10       226.10      8845.48
      2097152           20       477.85       477.96       477.90      8368.94
      4194304           10       995.52       995.61       995.56      8035.26

#-----------------------------------------------------------------------------
# Benchmarking Sendrecv 
# #processes = 4 
# ( 12 additional processes waiting in MPI_Barrier)
#-----------------------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]   Mbytes/sec
            0         1000         1.47         1.48         1.47         0.00
            1         1000         1.48         1.48         1.48         1.29
            2         1000         1.48         1.48         1.48         2.57
            4         1000         1.49         1.49         1.49         5.11
            8         1000         1.49         1.49         1.49        10.21
           16         1000         1.49         1.49         1.49        20.43
           32         1000         2.50         2.50         2.50        24.39
           64         1000         2.49         2.49         2.49        48.99
          128         1000         2.58         2.59         2.58        94.44
          256         1000         2.77         2.77         2.77       176.22
          512         1000         2.94         2.94         2.94       331.71
         1024         1000         3.31         3.32         3.32       588.29
         2048         1000         4.09         4.10         4.09       953.22
         4096         1000         4.81         4.82         4.81      1622.18
         8192         1000         6.47         6.47         6.47      2414.29
        16384         1000         8.17         8.18         8.17      3820.78
        32768         1000        11.16        11.16        11.16      5599.33
        65536          640        17.41        17.41        17.41      7178.03
       131072          320        38.47        38.50        38.49      6492.98
       262144          160        58.11        58.20        58.16      8591.14
       524288           80       122.21       122.61       122.41      8155.76
      1048576           40       230.15       230.80       230.50      8665.47
      2097152           20       480.45       484.35       482.38      8258.54
      4194304           10       981.19       997.81       991.26      8017.59

#-----------------------------------------------------------------------------
# Benchmarking Sendrecv 
# #processes = 8 
# ( 8 additional processes waiting in MPI_Barrier)
#-----------------------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]   Mbytes/sec
            0         1000         1.48         1.48         1.48         0.00
            1         1000         1.49         1.50         1.49         1.28
            2         1000         1.50         1.51         1.50         2.53
            4         1000         1.50         1.50         1.50         5.07
            8         1000         1.49         1.50         1.49        10.21
           16         1000         1.49         1.50         1.50        20.39
           32         1000         2.48         2.49         2.49        24.48
           64         1000         2.47         2.48         2.48        49.26
          128         1000         2.56         2.57         2.56        95.11
          256         1000         2.77         2.77         2.77       175.96
          512         1000         2.93         2.94         2.93       332.04
         1024         1000         3.27         3.28         3.27       595.65
         2048         1000         4.01         4.02         4.01       971.71
         4096         1000         4.70         4.71         4.71      1659.06
         8192         1000         6.25         6.27         6.26      2492.91
        16384         1000         8.13         8.15         8.14      3833.86
        32768         1000        10.96        10.96        10.96      5701.51
        65536          640        17.65        17.67        17.66      7075.26
       131072          320        38.54        38.61        38.57      6475.56
       262144          160        59.12        59.16        59.14      8451.15
       524288           80       124.31       124.48       124.39      8033.72
      1048576           40       240.90       241.83       241.40      8270.34
      2097152           20       473.70       476.05       474.83      8402.47
      4194304           10       954.29       990.51       972.37      8076.65

#-----------------------------------------------------------------------------
# Benchmarking Sendrecv 
# #processes = 16 
#-----------------------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]   Mbytes/sec
            0         1000         1.47         1.48         1.48         0.00
            1         1000         1.49         1.50         1.50         1.27
            2         1000         1.51         1.52         1.51         2.52
            4         1000         1.52         1.52         1.52         5.02
            8         1000         1.53         1.54         1.54         9.90
           16         1000         1.50         1.51         1.51        20.14
           32         1000         2.49         2.50         2.50        24.41
           64         1000         2.55         2.56         2.55        47.66
          128         1000         2.58         2.60         2.59        94.01
          256         1000         2.77         2.78         2.78       175.33
          512         1000         3.04         3.06         3.05       319.13
         1024         1000         3.41         3.43         3.42       569.40
         2048         1000         4.01         4.03         4.02       969.07
         4096         1000         4.75         4.77         4.76      1636.84
         8192         1000         6.32         6.35         6.34      2459.88
        16384         1000         8.08         8.10         8.09      3859.49
        32768         1000        10.97        10.99        10.98      5689.01
        65536          640        18.15        18.18        18.17      6874.08
       131072          320        38.33        38.44        38.37      6504.06
       262144          160        59.35        59.56        59.49      8394.48
       524288           80       123.75       124.46       124.11      8034.68
      1048576           40       238.82       244.25       241.77      8188.40
      2097152           20       478.90       500.09       488.32      7998.48
      4194304           10       956.80      1010.42       982.91      7917.52


# All processes entering MPI_Finalize

