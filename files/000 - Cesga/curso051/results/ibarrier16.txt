[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=7d1ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=171dab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1ca4ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=17e6ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=aefab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=8ffab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=238dab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=16ebab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=15c3ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=21f7ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=17b0ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=24a9ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=a6cab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1db2ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=1e80ab0
[-1] MPI startup(): Imported environment partly inaccesible. Map=0 Info=ab1ab0
[0] MPI startup(): Multi-threaded optimized library
[13] MPI startup(): shm data transfer mode
[14] MPI startup(): shm data transfer mode
[12] MPI startup(): shm data transfer mode
[15] MPI startup(): shm data transfer mode
[0] MPI startup(): shm data transfer mode
[11] MPI startup(): shm data transfer mode
[3] MPI startup(): shm data transfer mode
[5] MPI startup(): shm data transfer mode
[7] MPI startup(): shm data transfer mode
[10] MPI startup(): shm data transfer mode
[1] MPI startup(): shm data transfer mode
[2] MPI startup(): shm data transfer mode
[4] MPI startup(): shm data transfer mode
[6] MPI startup(): shm data transfer mode
[8] MPI startup(): shm data transfer mode
[9] MPI startup(): shm data transfer mode
[0] MPI startup(): Rank    Pid      Node name  Pin cpu
[0] MPI startup(): 0       25489    c7127      +1
[0] MPI startup(): 1       25490    c7127      +1
[0] MPI startup(): 2       25491    c7127      +1
[0] MPI startup(): 3       25492    c7127      +1
[0] MPI startup(): 4       25493    c7127      +1
[0] MPI startup(): 5       25494    c7127      +1
[0] MPI startup(): 6       25495    c7127      +1
[0] MPI startup(): 7       25496    c7127      +1
[0] MPI startup(): 8       25497    c7127      +1
[0] MPI startup(): 9       25498    c7127      +1
[0] MPI startup(): 10      25499    c7127      +1
[0] MPI startup(): 11      25500    c7127      +1
[0] MPI startup(): 12      25501    c7127      +1
[0] MPI startup(): 13      25502    c7127      +1
[0] MPI startup(): 14      25503    c7127      +1
[0] MPI startup(): 15      25504    c7127      +1
[0] MPI startup(): I_MPI_DEBUG=5
#------------------------------------------------------------
#    Intel (R) MPI Benchmarks 4.1 Update 1, MPI-1 part    
#------------------------------------------------------------
# Date                  : Mon Nov 14 11:24:16 2016
# Machine               : x86_64
# System                : Linux
# Release               : 2.6.32-573.12.1.el6.x86_64
# Version               : #1 SMP Mon Nov 23 12:55:32 EST 2015
# MPI Version           : 3.0
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# /opt/cesga/intel/impi/5.1.3.223/bin64/IMB-MPI1 Barrier

# Minimum message length in bytes:   0
# Maximum message length in bytes:   4194304
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# Barrier

#---------------------------------------------------
# Benchmarking Barrier 
# #processes = 2 
# ( 14 additional processes waiting in MPI_Barrier)
#---------------------------------------------------
 #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         1000         0.43         0.43         0.43

#---------------------------------------------------
# Benchmarking Barrier 
# #processes = 4 
# ( 12 additional processes waiting in MPI_Barrier)
#---------------------------------------------------
 #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         1000         0.81         0.81         0.81

#---------------------------------------------------
# Benchmarking Barrier 
# #processes = 8 
# ( 8 additional processes waiting in MPI_Barrier)
#---------------------------------------------------
 #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         1000         1.21         1.21         1.21

#---------------------------------------------------
# Benchmarking Barrier 
# #processes = 16 
#---------------------------------------------------
 #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         1000         2.28         2.28         2.28


# All processes entering MPI_Finalize

